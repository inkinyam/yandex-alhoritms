/*
1. ОТЧЕТ: https://contest.yandex.ru/contest/24414/run-report/138163040/
ОТЧЕТ V2: https://contest.yandex.ru/contest/24414/run-report/138223925/
ОТЧЕТ V3 https://contest.yandex.ru/contest/24414/run-report/138262263/ - убрала повторяющуюся сортировку в отдельную функцию
2. ПРИНЦИП РАБОТЫ
Основная функция findRelevantDocuments принимает на вход на списка: список строк с "документами", которые будут ранжироваться по релевантности, список строк с "запросами", по которым будут ранжироваться  "документы."

Первым делом нужно составить справочник всех слов во всех документах. Я выбрала структуру Мапов следующего вида:
    {
    слово: {
        индекс документа: количество повторений в строке
      }
    }
Данная структура позволяет быстро управлять добавлениями в нужный сегмент слов/включений в строках.
Для ее построения мы проходимся циклами, сначала по списку документов, далее по каждому слову в документе, проверяя, есть ли в индексе уже такое слово, если его нет, оно добавляется с количеством повторений 1, если оно было, прибавляем еще одно повторение. Функция buildSearchIndex возвращает "библиотеку" (searchIndex) всех слов во всех документах, далее мы работаем с ней.

Далее в основной функции, мы проходимся циклом по всем запросам, где выбираем все уникальные слова из запроса (для этого использую структуру Set, которая из коробки выбирает все уникальные вхождения в передаваемом массиве).
Далее проходим циклом по массиву уникальных слов из запроса и составляем новую мапу релевантностей документов, где:
- проверяем есть ли в индексе searchIndex слово из запроса, 
- если есть, добавляем к релевантностям соответствующих строк количество вхождений данного слова в строке.

После завершения вычисления релевантностей всех строк, начинаем готовить ответ для запроса.
В случае, если у нас найдено менее заданного в ответе числа строк (5), сортируем их по условиями (сначала релевантность по убыванию, потом индексы по возрастанию если релевантности равны)
В случае, если найдено более 5 релевантных строк, проходимся циклом по списку релевантностей, и собираем из них топ5 элементов. После каждого добавления сортируем по условиям список из топ5 элементов, чтобы корректно проверять попадает ли следующий элемент или нет.
Далее добавляем единичку к индексам, так как я оперировала индексами строк, а нужна нумерация с 1 как требует этого задача.


3. ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ
Данное решение корректно потому что:
- в алгоритме при построении searchIndex корректно учитывается количество вхождений слова в строку
- а при запросе учитываются только уникальные значения, для каждого уникального слова суммируем его вхождения в документ
- результаты правильно сортируются и ограничиваются 5 документами, как указано в  задании.
- высчитываются релевантности строк по каждому запросу отдельно.

4. ВРЕМЕННАЯ СЛОЖНОСТЬ

4.1. Время на обработку входных данных: 
1) Прочитать длину вх.документов - О(1)
2) Прочитать массив вх.документов - О(n)
3) Прочитать длину запросов - О(1)
4) Прочитать массив запросов - О(m)
Итого времязатраты на чтение запроса: O(n)+O(m)

4.2. Построение индекса:
Происходит при обработке всех слов в каждой строке и зависит от длины массива и длины строк, то есть зависит от количества слов во всех вх.документах.
При n- количестве строк вх.документов и w-количестве слов в каждой строке, временная сложность составления индекса составит O(n*w)

4.3. Обработка всех запросов
Разбитие запросов  на слова происходит за О(m), так как мы в любом случае должны пройти по всему массив и разбить предложения на слова.
Далее в каждом предложении выбираем уникальные слова для запроса, по одному разу по каждому слову в предложении придется пройти и добавить его в Set.Соответственно выбор уникальных элементов происходит за O(q), где q - количество слов в предложении.

Далее мы проходит по массиву уникальных элементов, который в худшем случае будет равен q (все слова = уникальны)  за О(q)
По каждому из слов проверяем в индексе наличие слова за О(1), 
далее проходя по списку документов,в которых встречалось слово (в худшем случае - О(n)- во всех, в лучшем - О(1)-в одном)
Записываем значение в новую мапу за О(1) для каждого.
Получается, что составление таблицы релевантности для всех запросов составит:
В худшем случае, когда в запросе все слова - уникальные, и каждое слово встречалось в каждом документе
временная сложность: О(m)*O(q)*О(q)*О(1)*О(n) ~ O(m*q*q*n)
m - количество строк запросов
q - количество слов в одном запросе:
    - сначала составляем уникальный список, 
    - потом по нему проходимся циклом
n - количество документов в которых попадалось слово


В лучшем случае, когда в запросе одно слово, и оно появлялось только в одном документе (или его вообще не было в индексе):
временная сложность: O(m)*O(1)*O(1)*O(1)*O(1) ~ O(m)

В среднем случае будем считать, что O(m)*O(q)*O(Uq)*O(Nw) где
m- количество строк запросов
q-количество слов в запросе
Uq -количество уникальных слов в запросе
Nw-среднее количество документов, содержащих слово

4.4. Составление ответа:
Составление массива из Мапы релевантных ответов - O(r)
В случае, если элементов меньше заданных 5, сортируем обычной сортировкой, которая занимает по времени О(5log5), что фактически можно считать константным временем, здесь и далее считаю что за О(1)
В случае, если элементов больше заданных 5, выполняется проходка за O(r) по всему массиву из результатов и выполняются
- проверка на длину массива top5 
- добавление в массив top5 за О(1), если меньше
- если длина массива = заданному кол-ву элементов, выполняется сортировка за  О(1)
- проверка элемента на удовлетворение условиям топ5 больше чем последний элемент в массиве за О(1)*2 (две проверки)
- замена элемента за О(1)
- сортировка опять же 5 элементов, О(1)
Итого, такой выбор элементов будет происходить за О(r)*(m*O(1)) времени, m -количество операций по перестановкам, сравнениям, сортировкам итд.. что можно считать за О(r) в среднем случае.
Общее время на составление ответа в таком случае займет О(r)+О(r)

Такое решение позволит управлять количеством выдачей 
Время выполнения алгоритма линейно зависит от количества слов в документах и запросах.


5. ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ
5.1. Входные данные:
O(n) - документы
O(m) - запросы.

5.2. Индекс
В связи со структурой поискового индекса, считаем, что индекс занимает:
-библиотека слов-  O(Uw), где Uw-количество уникальных слов во всех документах
- для каждого слова хранится индекс документов и количество раз попаданий, при этом можем считать, что слово встречается в K документов, при этом в худшем случае k может быть равно n, в среднем - k<n

Итого в худшем случае индекс занимает O(Uw)*O(n), в среднем- O(Uw)*O(k), k<n
n- все документы

5.3 Обработка запроса
Мапа релевантностей одного запроса хранит список релевантных строк, в лучшем случае О(1), в худшем - О(n), в среднем O(r), r < n
Массив уникальных слов из одного запроса -  в лучшем случае О(1), в худшем - О(q), в среднем O(Uq), Uq < q
Массив для сортировки  - O(r)
Добавился массив топ5 для каждого запроса = О(5)~ O(1)

Итого для обработки запроса : 
В худшем случае: O(n+q) - все документы релевантны, все слова в запросе - уникальны
В среднем: O(r+Uq) - r<n, Uq<q

5.4.Хранение результата
В результате у нас в худшем случае 5 элементов в каждом массиве из q запросов, итого O(5*q)


P.S. Пожалуйста напиши, если я слишком сильно расписываю, хочу понять минимально допустимый объем.
*/

const _readline = require("readline");

const _reader = _readline.createInterface({
  input: process.stdin,
});

const inputLines = [];
let curLine = 0;

_reader.on("line", (line) => {
  inputLines.push(line);
});

process.stdin.on("end", solve);
const RESULT_COUNTS = 5;
const buildSearchIndex = (documents) => {
  const searchIndex = new Map();
  for (let d = 0; d < documents.length; d++) {
    const words = documents[d].split(" ");
    for (const word of words) {
      const docMap = searchIndex.has(word) ? searchIndex.get(word) : new Map();
      docMap.set(d, (docMap.get(d) || 0) + 1);
      searchIndex.set(word, docMap);
    }
  }
  return searchIndex;
};

const sortElements = (a, b) => {
  const [indexA, valueA] = a;
  const [indexB, valueB] = b;
  return valueB - valueA || indexA - indexB;
};

const findRelevantDocuments = (documents, queries) => {
  const searchIndex = buildSearchIndex(documents);
  const results = [];

  for (const query of queries) {
    const relevance = new Map();
    const uniqueQueryWords = Array.from(new Set(query.split(" ")));

    for (const word of uniqueQueryWords) {
      if (searchIndex.has(word)) {
        for (const [d, count] of searchIndex.get(word)) {
          relevance.set(d, (relevance.get(d) || 0) + count);
        }
      }
    }
    const entries = Array.from(relevance.entries());
    let top5 = [];

    // если элементов меньше RESULT_COUNTS, сортировка выполняется на маленьком кол-ве данных за NlogN, даже в худшем случае - не страшно
    // но с учетом небольшого количества элементов, в нашем случае 5, фактически - за константное время
    if (entries.length < RESULT_COUNTS) {
      entries.sort(sortElements);
      top5 = entries;
    } else {
      for (let i = 0; i < entries.length; i++) {
        //собираем ТОП5 элементов, проходкой по всему списку за O(n) и константными проверками/перестановками (и короткими сортировками)
        const entry = entries[i];
        if (top5.length < RESULT_COUNTS) {
          top5.push(entry);

          if (top5.length === RESULT_COUNTS) {
            top5.sort(sortElements);
          }
        } else if (
          entry[1] > top5[RESULT_COUNTS - 1][1] ||
          (entry[1] === top5[RESULT_COUNTS - 1][1] &&
            entry[0] < top5[RESULT_COUNTS - 1][0])
        ) {
          top5[RESULT_COUNTS - 1] = entry;
          top5.sort(sortElements);
        }
      }
    }

    results.push(top5.map(([d]) => d + 1));
  }

  return results;
};

function solve() {
  const countDocuments = readInt();
  const documents = readArray(countDocuments);
  const countQuery = readInt();
  const queries = readArray(countQuery);

  const result = findRelevantDocuments(documents, queries);
  for (let i = 0; i < result.length; i++) {
    process.stdout.write(result[i].join(" ").toString());
    process.stdout.write("\n");
  }
}

function readInt() {
  const n = Number(inputLines[curLine]);
  curLine++;
  return n;
}

function readArray(counter) {
  const arr = [];
  for (let i = 0; i < counter; i++) {
    arr.push(inputLines[curLine++]);
  }
  return arr;
}
