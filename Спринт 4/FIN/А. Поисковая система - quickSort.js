/*
1. ОТЧЕТ: https://contest.yandex.ru/contest/24414/run-report/138163040/
ОТЧЕТ V2: https://contest.yandex.ru/contest/24414/run-report/138223925/
ОТЧЕТ v3:  https://contest.yandex.ru/contest/24414/run-report/138268522/ - вариант реализации с быстрой сортировкой.

2. ПРИНЦИП РАБОТЫ
Основная функция findRelevantDocuments принимает на вход на списка: список строк с "документами", которые будут ранжироваться по релевантности, список строк с "запросами", по которым будут ранжироваться  "документы."

Первым делом нужно составить справочник всех слов во всех документах. Я выбрала структуру Мапов следующего вида:
    {
    слово: {
        индекс документа: количество повторений в строке
      }
    }
Данная структура позволяет быстро управлять добавлениями в нужный сегмент слов/включений в строках.
Для ее построения мы проходимся циклами, сначала по списку документов, далее по каждому слову в документе, проверяя, есть ли в индексе уже такое слово, если его нет, оно добавляется с количеством повторений 1, если оно было, прибавляем еще одно повторение. Функция buildSearchIndex возвращает "библиотеку" (searchIndex) всех слов во всех документах, далее мы работаем с ней.

Далее в основной функции, мы проходимся циклом по всем запросам, где выбираем все уникальные слова из запроса (для этого использую структуру Set, которая из коробки выбирает все уникальные вхождения в передаваемом массиве).
Далее проходим циклом по массиву уникальных слов из запроса и составляем новую мапу релевантностей документов, где:
- проверяем есть ли в индексе searchIndex слово из запроса, 
- если есть, добавляем к релевантностям соответствующих строк количество вхождений данного слова в строке.

После завершения вычисления релевантностей всех строк, начинаем готовить ответ для запроса.
В случае, если у нас найдено менее заданного в ответе числа строк (5), сортируем их по условиями (сначала релевантность по убыванию, потом индексы по возрастанию если релевантности равны)
В случае, если найдено более 5 релевантных строк, используем быструю сортировку всех элементов. В функцию сортировки передаем метод сортировки sortElements.
Для сортировки выбираем рандомный элемент за pivot, сортируем левую часть (те элементы, которые должны быть больше опорного), далее проверяем, входит ли опорный элемент в правую часть, если да, то сортируем и ее.
В итоге первые K элементов будут отсортированы и находиться на своих местах.
Далее добавляем единичку к индексам, так как я оперировала индексами строк, а нужна нумерация с 1 как требует этого задача.


3. ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ
Данное решение корректно потому что:
- в алгоритме при построении searchIndex корректно учитывается количество вхождений слова в строку
- а при запросе учитываются только уникальные значения, для каждого уникального слова суммируем его вхождения в документ
- результаты правильно сортируются и ограничиваются 5 документами, как указано в  задании.
- высчитываются релевантности строк по каждому запросу отдельно.

4. ВРЕМЕННАЯ СЛОЖНОСТЬ

4.1. Время на обработку входных данных: 
1) Прочитать длину вх.документов - О(1)
2) Прочитать массив вх.документов - О(n)
3) Прочитать длину запросов - О(1)
4) Прочитать массив запросов - О(m)
Итого времязатраты на чтение запроса: O(n)+O(m)

4.2. Построение индекса:
Происходит при обработке всех слов в каждой строке и зависит от длины массива и длины строк, то есть зависит от количества слов во всех вх.документах.
При n- количестве строк вх.документов и w-количестве слов в каждой строке, временная сложность составления индекса составит O(n*w)

4.3. Обработка всех запросов
Разбитие запросов  на слова происходит за О(m), так как мы в любом случае должны пройти по всему массив и разбить предложения на слова.
Далее в каждом предложении выбираем уникальные слова для запроса, по одному разу по каждому слову в предложении придется пройти и добавить его в Set.Соответственно выбор уникальных элементов происходит за O(q), где q - количество слов в предложении.

Далее мы проходит по массиву уникальных элементов, который в худшем случае будет равен q (все слова = уникальны)  за О(q)
По каждому из слов проверяем в индексе наличие слова за О(1), 
далее проходя по списку документов,в которых встречалось слово (в худшем случае - О(n)- во всех, в лучшем - О(1)-в одном)
Записываем значение в новую мапу за О(1) для каждого.
Получается, что составление таблицы релевантности для всех запросов составит:
В худшем случае, когда в запросе все слова - уникальные, и каждое слово встречалось в каждом документе
временная сложность: О(m)*O(q)*О(q)*О(1)*О(n) ~ O(m*q*q*n)
m - количество строк запросов
q - количество слов в одном запросе:
    - сначала составляем уникальный список, 
    - потом по нему проходимся циклом
n - количество документов в которых попадалось слово


В лучшем случае, когда в запросе одно слово, и оно появлялось только в одном документе (или его вообще не было в индексе):
временная сложность: O(m)*O(1)*O(1)*O(1)*O(1) ~ O(m)

В среднем случае будем считать, что O(m)*O(q)*O(Uq)*O(Nw) где
m- количество строк запросов
q-количество слов в запросе
Uq -количество уникальных слов в запросе
Nw-среднее количество документов, содержащих слово

4.4. Составление ответа:
Составление массива из Мапы релевантных ответов - O(R)

Использую QuickSort с перестановкой для экономии памяти  и ускорения выдачи результатов.
Разбиение partition занимает О(r), r-количество элементов в relevance.
Рекурсивная обработка левой части занимает O(logK)
При условии, что k(количество элементов которые нужно вывести) достаточно мало, KlogK стремится к константному времени.
Итого в среднем получается сортировка O(r+K*logK)
В худшем случае (рандомный элемент каждый раз самый большой, например) - сортировка займет O(r*r+K*logK)
 

5. ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ
5.1. Входные данные:
O(n) - документы
O(m) - запросы.

5.2. Индекс
В связи со структурой поискового индекса, считаем, что индекс занимает:
-библиотека слов-  O(Uw), где Uw-количество уникальных слов во всех документах
- для каждого слова хранится индекс документов и количество раз попаданий, при этом можем считать, что слово встречается в K документов, при этом в худшем случае k может быть равно n, в среднем - k<n

Итого в худшем случае индекс занимает O(Uw)*O(n), в среднем- O(Uw)*O(k), k<n
n- все документы

5.3 Обработка запроса
Мапа релевантностей одного запроса хранит список релевантных строк, в лучшем случае О(1), в худшем - О(n), в среднем O(r), r < n
Массив уникальных слов из одного запроса -  в лучшем случае О(1), в худшем - О(q), в среднем O(Uq), Uq < q
Массив для сортировки  - O(r)
Память на стек вызовов рекурсии для сортировки O(log R). R - количество документов, содержащих слово из запроса.

Итого для обработки запроса : 
В худшем случае: O(n+q) - все документы релевантны, все слова в запросе - уникальны
В среднем: O(r+Uq) - r<n, Uq<q

5.4.Хранение результата
В результате у нас в худшем случае 5 элементов в каждом массиве из q запросов, итого O(5*q)


P.S. Пожалуйста напиши, если я слишком сильно расписываю, хочу понять минимально допустимый объем.
*/

const _readline = require("readline");

const _reader = _readline.createInterface({
  input: process.stdin,
});

const inputLines = [];
let curLine = 0;

_reader.on("line", (line) => {
  inputLines.push(line);
});

process.stdin.on("end", solve);

const RESULT_COUNTS = 5;

const buildSearchIndex = (documents) => {
  const searchIndex = new Map();
  for (let d = 0; d < documents.length; d++) {
    const words = documents[d].split(" ");
    for (const word of words) {
      const docMap = searchIndex.has(word) ? searchIndex.get(word) : new Map();
      docMap.set(d, (docMap.get(d) || 0) + 1);
      searchIndex.set(word, docMap);
    }
  }
  return searchIndex;
};
const sortElements = (a, b) => {
  const [indexA, valueA] = a;
  const [indexB, valueB] = b;
  return valueB - valueA || indexA - indexB;
};

const findRelevantDocuments = (documents, queries) => {
  const searchIndex = buildSearchIndex(documents);
  const results = [];

  for (const query of queries) {
    const relevance = new Map();
    const uniqueQueryWords = Array.from(new Set(query.split(" ")));

    for (const word of uniqueQueryWords) {
      if (searchIndex.has(word)) {
        for (const [d, count] of searchIndex.get(word)) {
          relevance.set(d, (relevance.get(d) || 0) + count);
        }
      }
    }

    const entries = Array.from(relevance.entries());

    if (entries.length <= RESULT_COUNTS) {
      entries.sort(sortElements);
    } else {
      quickSort(entries, RESULT_COUNTS, sortElements);
      entries.length = RESULT_COUNTS;
    }

    const top5 = entries.slice(0, RESULT_COUNTS).map(([d]) => d + 1);
    results.push(top5);
  }

  return results;
};

const quickSort = (arr, k, compare, left = 0, right = arr.length - 1) => {
  if (left >= right) return;

  const pivotIndex = partition(arr, left, right, compare);
  quickSort(arr, k, compare, left, pivotIndex - 1);

  if (pivotIndex < k - 1) {
    quickSort(arr, k, compare, pivotIndex + 1, right);
  }
};

const partition = (arr, left, right, compare) => {
  const pivotIndex = left + Math.floor(Math.random() * (right - left + 1));
  [arr[pivotIndex], arr[right]] = [arr[right], arr[pivotIndex]];

  const pivot = arr[right];
  let i = left;

  for (let j = left; j < right; j++) {
    if (compare(arr[j], pivot) <= 0) {
      [arr[i], arr[j]] = [arr[j], arr[i]];
      i++;
    }
  }

  [arr[i], arr[right]] = [arr[right], arr[i]];
  return i;
};

function solve() {
  const countDocuments = readInt();
  const documents = readArray(countDocuments);
  const countQuery = readInt();
  const queries = readArray(countQuery);

  const result = findRelevantDocuments(documents, queries);
  for (let i = 0; i < result.length; i++) {
    process.stdout.write(result[i].join(" ").toString());
    process.stdout.write("\n");
  }
}

function readInt() {
  const n = Number(inputLines[curLine]);
  curLine++;
  return n;
}

function readArray(counter) {
  const arr = [];
  for (let i = 0; i < counter; i++) {
    arr.push(inputLines[curLine++]);
  }
  return arr;
}
